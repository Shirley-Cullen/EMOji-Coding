{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from mosei_dataset import MOSEIDataset\n",
    "from tqdm import tqdm\n",
    "from tbje import TBJENew\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.nn import init\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"tensors.pkl\"\n",
    "train_dataset = MOSEIDataset(data_path, \"train\")\n",
    "val_dataset = MOSEIDataset(data_path, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIM = train_dataset[0][1].shape[1]\n",
    "TEXT_DIM = train_dataset[0][3].shape[1]\n",
    "AUDIO_DIM = train_dataset[0][4].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_squared(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Compute the coefficient of determination (r^2 score)\n",
    "    \"\"\"\n",
    "    y_true_np = y_true.cpu().detach().numpy()\n",
    "    y_pred_np = y_pred.cpu().detach().numpy()\n",
    "    ss_res = np.sum((y_true_np - y_pred_np) ** 2)\n",
    "    ss_tot = np.sum((y_true_np - np.mean(y_true_np)) ** 2)\n",
    "    return 1 - ss_res/ss_tot if ss_tot > 0 else 0.0\n",
    "\n",
    "def mae(preds: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculate the Mean Absolute Error (MAE) between predictions and labels.\n",
    "    \n",
    "    Args:\n",
    "        preds (torch.Tensor): 1D tensor of predictions.\n",
    "        labels (torch.Tensor): 1D tensor of ground truth values.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A scalar tensor containing the MAE.\n",
    "    \"\"\"\n",
    "    # Ensure both tensors are 1D\n",
    "    if preds.ndim != 1 or labels.ndim != 1:\n",
    "        raise ValueError(\"Both preds and labels must be 1D tensors.\")\n",
    "    return torch.mean(torch.abs(preds - labels))\n",
    "\n",
    "def pearson_corr(pred, label):\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    label = label.detach().cpu().numpy()\n",
    "    corr_matrix = np.corrcoef(pred, label)\n",
    "    # Extract Pearson's r value (off-diagonal element)\n",
    "    return corr_matrix[0, 1]\n",
    "\n",
    "def class_from_string(class_path: str, *args, **kwargs):\n",
    "    # Expecting class_path format \"module_name.ClassName\", returns the class object, not initiated object\n",
    "    module_name, class_name = class_path.rsplit(\".\", 1)\n",
    "    module = __import__(module_name, fromlist=[class_name])\n",
    "    cls = getattr(module, class_name)\n",
    "    return cls\n",
    "\n",
    "def general_epoch(model, dataloader, loss_fn, optimizer=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    Performs one epoch of training or evaluation for regression.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): the model to train/evaluate.\n",
    "        dataloader (DataLoader): DataLoader for the current dataset (train or validation).\n",
    "        loss_fn (callable): the loss function.\n",
    "        optimizer (torch.optim.Optimizer or None): optimizer. If None, evaluation mode.\n",
    "        device (str): device to run the computation on.\n",
    "    \n",
    "    Returns:\n",
    "        epoch_loss (float): average loss over the epoch.\n",
    "        epoch_r (float): r-squared value over the epoch.\n",
    "    \"\"\"\n",
    "    if optimizer is not None:\n",
    "        model.train()  # training mode\n",
    "    else:\n",
    "        model.eval()   # evaluation mode\n",
    "    model = model.to(device=device, dtype=torch.float32)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_samples = 0\n",
    "    target_index = 0\n",
    "\n",
    "    for *inputs, labels in tqdm(dataloader): \n",
    "        inputs = [inputs[1].to(device), inputs[3].to(device), inputs[4].to(device)]\n",
    "        labels = labels[:, target_index].to(device, dtype=torch.float32) # regression targets\n",
    "\n",
    "        outputs = model(*inputs)\n",
    "        loss = loss_fn(outputs, labels.unsqueeze(1))\n",
    "        \n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            print(\"Loss:\", loss.item())\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    print(name, param.grad.norm().item())\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        all_preds.append(outputs.squeeze())\n",
    "        all_labels.append(labels)\n",
    "        total_samples += inputs[0].size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / total_samples\n",
    "    all_preds = torch.nan_to_num(torch.cat(all_preds), nan=0.0)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    epoch_r = pearson_corr(all_preds, all_labels)\n",
    "    epoch_mae = mae(all_preds, all_labels)\n",
    "    epoch_r2 = r_squared(all_preds, all_labels)\n",
    "    \n",
    "    return epoch_loss, epoch_r, epoch_mae, epoch_r2\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, loss_fn, optimizer, workdir,\n",
    "                device='cuda', num_epochs=25, patience=5):\n",
    "    \"\"\"\n",
    "    Train the regression model with mini-batch training, per-epoch validation, and early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): the model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training set.\n",
    "        valid_loader (DataLoader): DataLoader for the validation set.\n",
    "        loss_fn (callable): loss function.\n",
    "        optimizer (torch.optim.Optimizer): optimizer.\n",
    "        device (str): device to use.\n",
    "        num_epochs (int): maximum number of epochs.\n",
    "        patience (int): epochs to wait before early stopping.\n",
    "    \n",
    "    Returns:\n",
    "        model: the trained model (best state).\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improve = 0\n",
    "    train_loss_trace, train_r_trace, val_loss_trace, val_r_trace = [], [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss, train_r, train_mae, train_r2 = general_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "        train_loss_trace.append(train_loss)\n",
    "        train_r_trace.append(train_r)\n",
    "        print(f\"Train Loss: {train_loss:.4f} r: {train_r:.4f} MAE: {train_mae:.4f} R^2: {train_r2}\")\n",
    "        \n",
    "        # Validation phase\n",
    "        val_loss, val_r, val_mae, val_r2 = general_epoch(model, valid_loader, loss_fn, optimizer=None, device=device)\n",
    "        val_loss_trace.append(val_loss)\n",
    "        val_r_trace.append(val_r)\n",
    "        print(f\"Val   Loss: {val_loss:.4f} r: {val_r:.4f} MAE: {val_mae:.4f} R^2: {val_r2}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improve = 0\n",
    "            print(\"Validation loss improved, saving model...\")\n",
    "        else:\n",
    "            epochs_without_improve += 1\n",
    "            print(f\"No improvement for {epochs_without_improve} epoch(s).\")\n",
    "        torch.save(model.state_dict(), f\"{workdir}/epoch{epoch}_valloss{val_loss:.2f}_valmae{val_mae:.2f}.pt\")\n",
    "        \n",
    "        if epochs_without_improve >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    return model\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.kaiming_uniform_(m.weight, a=0, mode='fan_in', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERNAL_DIM = 256\n",
    "N_HEADS = 8\n",
    "MLP_DIM = 1024\n",
    "N_LAYERS = 2\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-6\n",
    "WEIGHT_DECAY = 0\n",
    "WORKDIR = \"tbje-test-round\"\n",
    "EPOCHS = 50\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TBJENew(VIDEO_DIM, TEXT_DIM, AUDIO_DIM, INTERNAL_DIM, N_HEADS, MLP_DIM, N_LAYERS)\n",
    "model.apply(initialize_weights)\n",
    "train_dataloader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, BATCH_SIZE, shuffle=True)\n",
    "loss_fn = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/511 [00:00<06:51,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1643686294555664\n",
      "vid_initial_proj.weight 0.29811257123947144\n",
      "txt_initial_proj.weight 0.27019548416137695\n",
      "aud_initial_proj.weight 0.09652508050203323\n",
      "vid_proj_ln.weight 0.018630361184477806\n",
      "txt_proj_ln.weight 0.02125122956931591\n",
      "aud_proj_ln.weight 0.01583101414144039\n",
      "block.vid_ln1.weight 0.010016562417149544\n",
      "block.txt_ln1.weight 0.006834547501057386\n",
      "block.aud_ln1.weight 0.009948560036718845\n",
      "block.vid_cross_attn.in_proj_weight 0.10664653033018112\n",
      "block.vid_cross_attn.out_proj.weight 0.04885974898934364\n",
      "block.txt_cross_attn.in_proj_weight 0.30084332823753357\n",
      "block.txt_cross_attn.out_proj.weight 0.15511515736579895\n",
      "block.aud_cross_attn.in_proj_weight 0.11425814032554626\n",
      "block.aud_cross_attn.out_proj.weight 0.051280345767736435\n",
      "block.vid_ln2.weight 0.010638553649187088\n",
      "block.txt_ln2.weight 0.01849421299993992\n",
      "block.aud_ln2.weight 0.011234266683459282\n",
      "block.vid_mlp_in.weight 0.11905863881111145\n",
      "block.vid_mlp_out.weight 0.21737779676914215\n",
      "block.txt_mlp_in.weight 0.20379133522510529\n",
      "block.txt_mlp_out.weight 0.3452044427394867\n",
      "block.aud_mlp_in.weight 0.11462240666151047\n",
      "block.aud_mlp_out.weight 0.220499649643898\n",
      "block.vid_ln3.weight 0.006465076468884945\n",
      "block.txt_ln3.weight 0.004896948579698801\n",
      "block.aud_ln3.weight 0.005604472476989031\n",
      "block.vid_slf_attn.in_proj_weight 0.13305270671844482\n",
      "block.vid_slf_attn.out_proj.weight 0.06630492210388184\n",
      "block.txt_slf_attn.in_proj_weight 0.11622628569602966\n",
      "block.txt_slf_attn.out_proj.weight 0.05664566159248352\n",
      "block.aud_slf_attn.in_proj_weight 0.12506937980651855\n",
      "block.aud_slf_attn.out_proj.weight 0.0632915049791336\n",
      "final_proj1.weight 0.5576340556144714\n",
      "final_proj2.weight 0.17179954051971436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/511 [00:01<11:08,  1.31s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function 'MmBackward0' returned nan values in its 1th output.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mWORKDIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPATIENCE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[110], line 125\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, valid_loader, loss_fn, optimizer, workdir, device, num_epochs, patience)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m train_loss, train_r, train_mae, train_r2 \u001b[38;5;241m=\u001b[39m \u001b[43mgeneral_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m train_loss_trace\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m    127\u001b[0m train_r_trace\u001b[38;5;241m.\u001b[39mappend(train_r)\n",
      "Cell \u001b[1;32mIn[110], line 77\u001b[0m, in \u001b[0;36mgeneral_epoch\u001b[1;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 77\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\sjfnx\\anaconda3\\envs\\mmml\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sjfnx\\anaconda3\\envs\\mmml\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sjfnx\\anaconda3\\envs\\mmml\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Function 'MmBackward0' returned nan values in its 1th output."
     ]
    }
   ],
   "source": [
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    train_model(model, train_dataloader, val_dataloader, loss_fn, optimizer, WORKDIR, num_epochs=EPOCHS, patience=PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
